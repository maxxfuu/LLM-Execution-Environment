{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before installing replicate, ensure a Python environmnet is setup: \n",
    "\n",
    "#! conda create -n <name_of_env> \n",
    "#! conda activate <name_of_env> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and dependencies\n",
    "import replicate, os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "replicate_api_token = os.getenv(\"RREPLICATE_API_TOKEN\") \n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = replicate_api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a unit test for the python code, def dummy(a, b): return a + b print(dummy(2, 5))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = {\n",
    "#     \"prompt\": f'{prompt}',\n",
    "#     \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYour whole output should only be code that is interpretable by the interpreter as described by the user. Your output should be ran by an interpreter with no errors.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "# }\n",
    "\n",
    "# output = replicate.run(\n",
    "#     \"meta/meta-llama-3-70b-instruct\",\n",
    "#     input=input\n",
    "# )\n",
    "# print(\"\".join(output))\n",
    "\n",
    "# # Your whole output should only be code that is interpretable by the interpreter as described by the user. Your output should be ran by an interpreter with no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReplicateError",
     "evalue": "ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 5\u001b[0m output \u001b[39m=\u001b[39m replicate\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mmeta/meta-llama-3-8b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(output))\n\u001b[1;32m     10\u001b[0m \u001b[39m#=> \" there were 3 llamas. They were very friendly and they d...\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMEE/lib/python3.12/site-packages/replicate/client.py:157\u001b[0m, in \u001b[0;36mClient.run\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     ref: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    150\u001b[0m     \u001b[39minput\u001b[39m: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams: Unpack[\u001b[39m\"\u001b[39m\u001b[39mPredictions.CreatePredictionParams\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    152\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Any, Iterator[Any]]:  \u001b[39m# noqa: ANN401\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    Run a model and wait for its output.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m run(\u001b[39mself\u001b[39;49m, ref, \u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMEE/lib/python3.12/site-packages/replicate/run.py:44\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     40\u001b[0m     prediction \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mpredictions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     41\u001b[0m         version\u001b[39m=\u001b[39mversion_id, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m \u001b[39mor\u001b[39;00m {}, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[39melif\u001b[39;00m owner \u001b[39mand\u001b[39;00m name:\n\u001b[0;32m---> 44\u001b[0m     prediction \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mpredictions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     45\u001b[0m         model\u001b[39m=\u001b[39;49m(owner, name), \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m \u001b[39mor\u001b[39;49;00m {}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid argument: \u001b[39m\u001b[39m{\u001b[39;00mref\u001b[39m}\u001b[39;00m\u001b[39m. Expected model, version, or reference in the format owner/name or owner/name:version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMEE/lib/python3.12/site-packages/replicate/model.py:310\u001b[0m, in \u001b[0;36mModelsPredictions.create\u001b[0;34m(self, model, input, **params)\u001b[0m\n\u001b[1;32m    307\u001b[0m url \u001b[39m=\u001b[39m _create_prediction_url_from_model(model)\n\u001b[1;32m    308\u001b[0m body \u001b[39m=\u001b[39m _create_prediction_body(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 310\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    311\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    312\u001b[0m     url,\n\u001b[1;32m    313\u001b[0m     json\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[39mreturn\u001b[39;00m _json_to_prediction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client, resp\u001b[39m.\u001b[39mjson())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMEE/lib/python3.12/site-packages/replicate/client.py:87\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, method: \u001b[39mstr\u001b[39m, path: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m httpx\u001b[39m.\u001b[39mResponse:\n\u001b[1;32m     86\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mrequest(method, path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 87\u001b[0m     _raise_for_status(resp)\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMEE/lib/python3.12/site-packages/replicate/client.py:367\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_for_status\u001b[39m(resp: httpx\u001b[39m.\u001b[39mResponse) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m400\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m600\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m         \u001b[39mraise\u001b[39;00m ReplicateError\u001b[39m.\u001b[39mfrom_response(resp)\n",
      "\u001b[0;31mReplicateError\u001b[0m: ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing."
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"prompt\": f'{prompt}'\n",
    "}\n",
    "\n",
    "output = replicate.run(\n",
    "    \"meta/meta-llama-3-8b\",\n",
    "    input=input\n",
    ")\n",
    "print(\"\".join(output))\n",
    "#=> \" there were 3 llamas. They were very friendly and they d..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stream the output in real-time \n",
    "\n",
    "# Prompts \n",
    "\n",
    "# input = {\n",
    "#     \"prompt\": f'{prompt}',\n",
    "#     #\"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "#     \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYour whole output should only be code that is interpretable by the interpreter as described by the user. Your output should be ran by an interpreter with no errors.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "# }\n",
    "\n",
    "# for event in replicate.stream(\n",
    "#     \"meta/meta-llama-3-70b-instruct\",\n",
    "#     input=input\n",
    "# ):\n",
    "#     print(event, end=\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output = output \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
